#!/bin/bash
#This file contains sample commands
#qperf
./qperf -v 172.17.0.4 udp_bw udp_lat
./qperf -t 60 -v 172.17.0.4 tcp_bw tcp_lat
./qperf -oo msg_size:1:64K:*2   172.17.0.4 tcp_bw tcp_lat

#udpsender
installed in /opt/tools/million/
 ./udpsender 172.17.0.4:4321
 ./udpreceiver1 0.0.0.0:4321


 taskset -c 1,2 ./udpsender   172.17.0.4:4321 172.17.4:4321
 taskset -c 1 ./udpreceiver1 0.0.0.0:4321

#you can watch traffic on both
watch sar -n DEV 1 3


#MTR
docker exec -it -u root 286365ce8c18 mtr -s 1472 -w -B 0 172.17.0.4
mtr --report ipaddress/fqdn
# -s size in bytes –w (wide output) 0 order

#The four columns Last, Avg, Best, and Wrst are all measurements of latency in milliseconds (e.g. ms). Last is the latency of the
#last packet sent, Avg is average latency of all packets, while Best and Wrst display the best (shortest) and worst (longest) round
#trip time for a packet to this host. In most cases, the average (Avg) column should be the focus of your attention.

#The final column, StDev, provides the standard deviation of the latencies to each host.
#The higher the standard deviation, the greater the difference is between measurements of latency. Standard deviation allows
#you to assess if the mean (average) provided represents the true center of the data set, or has been skewed by some sort of
#phenomena or measurement error.
#If the standard deviation is high, the latency measurements were inconsistent. After averaging the latencies of the 10 packets
#sent, the average looks normal but may in fact not represent the data very well. If the standard deviation is high, take a look at the
#best and worst latency measurements to make sure the average is a good representation of the actual latency and not the result
#of too much fluctuation.


#Netperf
First you need to start  /usr/local/bin/./netserver on host with
#At client, run sample command

Netperf -L 172.17.0.4 100 -H 172.17.0.3 -c -C -l 100 TCP_STREAM -- -m 64G

#-L local ip, -H netserver ip, -c cpu -C remote cpu -l testlenght above is 100 seconds TCP_STREAM as 64 gb
#

#iperf3
#Server
#start server
iperf3 -s

#start server daemon
iperf3 -s -D

#start server on port 5003:
iperf3 -s -p 5003

#start using 0.5 second interval times:
iperf3 -s -i 0.5

#Start server with larger TCP window, and in daemon mode
iperf -s -w 32M -D / iperf3 -s -D
#Start UDP server on port 5003, and give 1 sec interval reports. Note that for iperf3 the -u option is passed to the server from the

client.
iperf -i1 -u -s -p 5003 / iperf3 -s -p 5003

#Client Commands
#Run a 30 second test, giving results every 1 second:
iperf3 -c <dst-ip> -i 1 -t 30

#Run a test from remotehost to localhost:
iperf3 -c <dst-ip> -i 1 -t 20 -R

#Run a test with 4 parallel streams, and with a 32M TCP buffer
iperf3 -c <dst-ip> -w 32M -P 4

#Run a 200 Mbps UDP test:
iperf3 -c <dst-ip> -u -i 1 -b 200M
iperf3 -c <dst-ip> -O 2 -i 0.5

#Using the -Z flag enables the Zero Copy mode, which uses the sendfile() system call. This uses much less CPU to put the data

on the PCI bus.
iperf3 -c <dst-ip> -Z

#Output the results in JSON format using the -J flag for easy parsing by most coding languages. This output is printed after the

entire test finishes running.
iperf3 -c <dst-ip> -J
#Set the CPU affinity for the sender (-A 2) or the sender, receiver (-A 2,3), where the cores are numbered starting at 0. This has

the same effect as running numactl -C 4 iperf3.
iperf3 -c <dst-ip> -A 2,3
#Options
-w TCP window size
-u TCP vs UDP procotol
-b Target throughput rateer
-Z -A CPU utilization,
-O Data time sets used for average calculations
-P Concurrent connections on port

# Run tests to multiple interfaces at once, and label the lines to indicate which test is which
iperf3 -c 192.168.12.12 -T s1 & iperf3 -c 192.168.12.13 -T s2
# Output the results in JSON format for easy parsing.
iperf3 -c remotehost -J
#Set the CPU affinity for the sender,receiver (cores are numbered from 0). This has the same affect as doing 'numactl -C 4 iperf3'

on both client and server.
iperf3 -A 4,4 -c remotehost
# Run 2 streams on 2 different cores, and label each using the "-T" flag.

iperf3 -c 10.20.1.20 -A2,2 -T "1" & ; iperf3 -c 10.20.1.20 -p 5400 -A3,3 -T "2" &



#NETem with Traffic Control`
tc qdisc add dev br-fw-admin root netem (adds interface br-fw)
tc qdisc change dev br-fw-admin root netem corrupt 1 (adds corruption)
tc -s qdisc (shows status)
tc qdisc del dev br-fw-admin root netem (deletes the device and corruption)
tc qdisc add dev eth0 root netem rate 5kbit 20 100 5
   # delay  all  outgoing packets on device eth0 with a rate of 5kbit, a per packet
   # overhead of 20 byte, a cellsize of 100 byte  and a per celloverhead of 5 byte

#nuttcp

nuttcp -S (defaults to port 5000)

#Start server on port 5003:
nuttcp -S -P 5003

#Start server on the default port for IPv6:
nuttcp -s -6

#Tests
#10 second test, giving results every second:
nuttcp -i 1 <server>

#Increase time with T
-T30 = 30 sec, -T1m = 1 min, and -T1h = 1 hr

#Set the window in Megabytes (-w5m) or Gigabytes (-w5g):
nuttcp -w32 -ws32 <server>

#The default uses TCP, can test with UDP with 8192 byte datagrams (default):
nuttcp -u <server>

#Run a 10 second UDP test at 50Mbps, and report throughput and packet loss:
nuttcp -u -R50m <server>

#Run a 10 second UDP test with 1200 byte datagrams:
nuttcp -u -l1200 <server>

#Receiver Commands

#Run a 10 second test in reverse (server → client), giving results every 1 second:
nuttcp -r -i 1 <server>


#nuttcp includes a burst mode for UDP, which is useful for finding network paths constrained a lack of buffering. To do this, set an
#instantaneous data rate with optional packet burst. Specify the rates in Kbps (-R100), Mbps (-R5m), Gbps (-R5g) or PPS (-
#R1000p). The burst is just a count of packets.

#Send 300 Mbps of UDP at an instantaneous rate in bursts of 20 packets for 5 seconds:
nuttcp -u -Ri300m/20 -i 1 -T5 <server>

#This test may show a small amount of loss. If you increase the burst size at the same instantaneous rate, it will increase the
#stress to the network buffering. This will progressively show more loss as the transit device overflows its buffers.

#On 10GbE network with 9000 byte MTU, send 8972 byte UDP datagrams at an instantaneous rate of 300 Mbps, and bursting at
#100 packets. This should be lossless:
nuttcp -l8972 -T30 -u -w4m -Ri300m/100 -i1 <server>

#Again, increasing the burst value and retesting eventually shows where buffer exhaustion happens in the network:

#10G UDP Testing
# nuttcp may be the best tool for high speed UDP testing. Able to create a full 10 Gbps traffic flow using UDP, but requires jumbo
# MTU size packets (9K):
nuttcp -l8972 -T30 -u -w4m -Ru -i1 <server>

#CPU Affinity - can specify core.

#Can send multiple streams, using CPU affinity binding to make sure processes do not end up on the same CPU core. Be sure
#to start the servers on the intended destination ports. Also, using the statistics output identifier helps to label the results with an s1
#or s2:
nuttcp -i1 -xc 2/2 -Is1 -u -Ru -l8972 -w4m -p 5500 <server> & \
nuttcp -i1 -xc 3/3 -Is2 -u -Ru -l8972 -w4m -p 5501 <server>


#hping3 - uses tcp by defualt
hping3 -S --flood -V ipaddress/website
hping3 -1 -c 1 172.17.0.4
-0 raw ip, -1 ICMP, -2 UDP , -8 (Scan ) -9 Listen
-c 100000 = Number of packets to send.
-d 120 = Size of each packet that was sent to target machine.
-S = I am sending SYN packets only.  -A (Ack) -R (RST) -F (FIN) -P (PUSH) -U (URG) - X (XMAS) -Y (YMAS)
-w 64 = TCP window size.
-p 21 = Destination port (21 being FTP port). You can use any port here.
--flood = Sending packets as fast as possible, without taking care to show incoming replies. Flood mode.
--rand-source = Using Random Source IP Addresses. You can also use -a or –spoof to hide hostnames. See MAN page below.
-i  --interval  wait (uX for X microseconds, for example -i u1000)
      --fast      alias for -i u10000 (10 packets for second)
      --faster    alias for -i u1000 (100 packets for second)
      --flood      sent packets as fast as possible. Don't show replies.
-I  --interface interface name (otherwise default routing interface)
-V  --verbose   verbose mode
-D  --debug     debugging info

   #pkgen tar in /opt/tar_files (not invoked)
#./pktgen_sample02.sh  -i br-ex -s 16192 -m 00:8c:fa:5a:e3:da  -t 8 -f 6 -l 8192 -v -d 10.5.248.20



